{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNLOH-LXeBYu",
        "outputId": "2dc807d8-f3a1-4245-b288-0a6a158fbf6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-agent: *\n",
            "  Allow:\n",
            "    /search/$\n",
            "  Disallow:\n",
            "    /api\n",
            "    /asset-manifest.json\n",
            "    /search/\n",
            "    /home/search?q=\n",
            "\n",
            "User-agent: anthropic-ai\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: ChatGPT-User\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: ClaudeBot\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: Claude-Web\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: cohere-ai\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: GPTBot\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: PerplexityBot\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "User-agent: Bytespider\n",
            "  Allow:\n",
            "  Disallow:\n",
            "    /\n",
            "\n",
            "Sitemaps found:\n",
            "  https://www.aljazeera.com/sitemap.xml\n",
            "  https://www.aljazeera.com/news-sitemap.xml\n",
            "  https://www.aljazeera.com/sitemaps/article-archive.xml\n",
            "  https://www.aljazeera.com/sitemaps/article-new.xml\n",
            "  https://www.aljazeera.com/sitemaps/video-archive.xml\n",
            "  https://www.aljazeera.com/sitemaps/video-new.xml\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_robots_txt(url):\n",
        "    if not url.endswith('/'):\n",
        "        url += '/'\n",
        "    robots_url = url + 'robots.txt'\n",
        "    response = requests.get(robots_url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch robots.txt: HTTP {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def parse_robots_txt(content):\n",
        "    rules = {}\n",
        "    sitemaps = []\n",
        "    current_user_agent = None\n",
        "\n",
        "    for line in content.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "\n",
        "        if line.lower().startswith('user-agent:'):\n",
        "            current_user_agent = line.split(':', 1)[1].strip()\n",
        "            if current_user_agent not in rules:\n",
        "                rules[current_user_agent] = {'allow': [], 'disallow': []}\n",
        "        elif line.lower().startswith('allow:') and current_user_agent:\n",
        "            path = line.split(':', 1)[1].strip()\n",
        "            rules[current_user_agent]['allow'].append(path)\n",
        "        elif line.lower().startswith('disallow:') and current_user_agent:\n",
        "            path = line.split(':', 1)[1].strip()\n",
        "            rules[current_user_agent]['disallow'].append(path)\n",
        "        elif line.lower().startswith('sitemap:'):\n",
        "            sitemap_url = line.split(':', 1)[1].strip()\n",
        "            sitemaps.append(sitemap_url)\n",
        "\n",
        "    return rules, sitemaps\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.aljazeera.com\"\n",
        "    content = fetch_robots_txt(url)\n",
        "    if content:\n",
        "        rules, sitemaps = parse_robots_txt(content)\n",
        "        for agent, paths in rules.items():\n",
        "            print(f\"User-agent: {agent}\")\n",
        "            print(\"  Allow:\")\n",
        "            for p in paths['allow']:\n",
        "                print(f\"    {p}\")\n",
        "            print(\"  Disallow:\")\n",
        "            for p in paths['disallow']:\n",
        "                print(f\"    {p}\")\n",
        "            print()\n",
        "        print(\"Sitemaps found:\")\n",
        "        for sitemap in sitemaps:\n",
        "            print(f\"  {sitemap}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "def fetch_sitemap(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch sitemap: HTTP {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def parse_sitemap_index(xml_content):\n",
        "    sitemap_urls = []\n",
        "    try:\n",
        "        root = ET.fromstring(xml_content)\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        for sitemap in root.findall('ns:sitemap', namespace):\n",
        "            loc = sitemap.find('ns:loc', namespace)\n",
        "            if loc is not None:\n",
        "                sitemap_urls.append(loc.text)\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"Error parsing sitemap XML: {e}\")\n",
        "    return sitemap_urls\n",
        "\n",
        "def parse_sitemap(xml_content):\n",
        "    urls = []\n",
        "    try:\n",
        "        root = ET.fromstring(xml_content)\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        for url in root.findall('ns:url', namespace):\n",
        "            loc = url.find('ns:loc', namespace)\n",
        "            if loc is not None:\n",
        "                urls.append(loc.text)\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"Error parsing sitemap XML: {e}\")\n",
        "    return urls\n",
        "\n",
        "def save_urls_to_txt(urls, filename):\n",
        "    try:\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"Saved {len(urls)} URLs to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving URLs to file: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sitemap_index_url = \"https://www.aljazeera.com/sitemap.xml\"\n",
        "    sitemap_index_content = fetch_sitemap(sitemap_index_url)\n",
        "\n",
        "    if sitemap_index_content:\n",
        "        all_urls = []\n",
        "        sitemap_urls = parse_sitemap_index(sitemap_index_content)\n",
        "\n",
        "        if sitemap_urls:\n",
        "            print(f\"Found {len(sitemap_urls)} sitemap URLs in sitemap index.\")\n",
        "            for i, sm_url in enumerate(sitemap_urls):\n",
        "                print(f\"Fetching URLs from sitemap {i+1}: {sm_url}\")\n",
        "                sm_content = fetch_sitemap(sm_url)\n",
        "                if sm_content:\n",
        "                    urls = parse_sitemap(sm_content)\n",
        "                    print(f\"Found {len(urls)} URLs in sitemap {i+1}\")\n",
        "                    all_urls.extend(urls)\n",
        "        else:\n",
        "            print(\"This is a regular sitemap (not an index).\")\n",
        "            urls = parse_sitemap(sitemap_index_content)\n",
        "            all_urls.extend(urls)\n",
        "\n",
        "        os.makedirs(\"sitemaps_files\", exist_ok=True)\n",
        "        save_urls_to_txt(all_urls, os.path.join(\"sitemaps_files\", \"all_sitemaps_url.txt\"))"
      ],
      "metadata": {
        "id": "5FkKhDsqnVUs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "rss_url = \"https://www.aljazeera.com/xml/rss/all.xml\"\n",
        "\n",
        "response = requests.get(rss_url)\n",
        "rss_content = response.content\n",
        "\n",
        "root = ET.fromstring(rss_content)\n",
        "\n",
        "channel = root.find('channel')\n",
        "items = channel.findall('item')\n",
        "\n",
        "for item in items[:5]:\n",
        "    title = item.find('title').text\n",
        "    link = item.find('link').text\n",
        "    pub_date = item.find('pubDate').text\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Link: {link}\")\n",
        "    print(f\"Published: {pub_date}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP1vUVyt7Jx3",
        "outputId": "136d731e-c014-46f2-9478-8bac50be9bbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Europe’s political centre holds in weekend of elections\n",
            "Link: https://www.aljazeera.com/video/inside-story/2025/5/19/europes-political-centre-holds-in-weekend-of-elections?traffic_source=rss\n",
            "Published: Mon, 19 May 2025 19:23:15 +0000\n",
            "--------------------------------------------------\n",
            "Title: Former President Bolsonaro’s coup trial opens in Brazil\n",
            "Link: https://www.aljazeera.com/news/2025/5/19/former-president-bolsonaros-coup-trial-opens-in-brazil?traffic_source=rss\n",
            "Published: Mon, 19 May 2025 19:22:13 +0000\n",
            "--------------------------------------------------\n",
            "Title: British presenter Gary Lineker steps down over anti-Semitism row\n",
            "Link: https://www.aljazeera.com/program/newsfeed/2025/5/19/british-presenter-gary-lineker-steps-down-over-anti-semitism-row?traffic_source=rss\n",
            "Published: Mon, 19 May 2025 18:34:29 +0000\n",
            "--------------------------------------------------\n",
            "Title: Pope Leo XIV meets US VP JD Vance\n",
            "Link: https://www.aljazeera.com/program/newsfeed/2025/5/19/pope-leo-xiv-meets-us-vp-jd-vance?traffic_source=rss\n",
            "Published: Mon, 19 May 2025 17:46:56 +0000\n",
            "--------------------------------------------------\n",
            "Title: Measure targeting pro-Palestine NGOs disappears from US tax bill\n",
            "Link: https://www.aljazeera.com/news/2025/5/19/measure-targeting-pro-palestine-ngos-disappears-from-us-tax-bill?traffic_source=rss\n",
            "Published: Mon, 19 May 2025 17:28:10 +0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.aljazeera.com/api\"\n",
        "#url = \"https://jsonplaceholder.typicode.com/posts\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"MyCrawler/1.0\",\n",
        "    \"Accept\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    for post in data[:5]:\n",
        "        print(f\"Post ID: {post['id']}, Title: {post['title']}\")\n",
        "else:\n",
        "    print(f\"Failed to get data, status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTtTG_Qp_cbP",
        "outputId": "b269dd37-d772-4c00-bbad-a100cd8b6e97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get data, status code: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.aljazeera.com/api/endpoint-specific\"  # حط رابط الـ API الصحيح هنا\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"MyCrawler/1.0\",\n",
        "    \"Accept\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        data = response.json()\n",
        "        print(data)\n",
        "    except ValueError:\n",
        "        print(\"Response is not in JSON format.\")\n",
        "else:\n",
        "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9jTOhHVBUMZ",
        "outputId": "d636169f-74a9-421d-b9c8-7f45139fbd5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data. Status code: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.robotparser import RobotFileParser\n",
        "\n",
        "robots_url = \"https://www.aljazeera.com/robots.txt\"\n",
        "\n",
        "rp = RobotFileParser()\n",
        "rp.set_url(robots_url)\n",
        "rp.read()\n",
        "\n",
        "# crawl_delay\n",
        "delay = rp.crawl_delay(\"*\")\n",
        "print(f\"Crawl-delay is: {delay} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8kC3tHSCYON",
        "outputId": "96d6fe46-9505-41b5-a3c1-0893d0ac25f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawl-delay is: None seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy twisted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2RCs-ibJy6h",
        "outputId": "6c58c2c6-a69c-435a-f4d1-80066a1fab1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading scrapy-2.13.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting twisted\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pydispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope-interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted) (25.3.0)\n",
            "Collecting automat>=24.8.0 (from twisted)\n",
            "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from twisted)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from twisted)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from twisted)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted) (4.13.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.11/dist-packages (from hyperlink>=17.1.1->twisted) (3.10)\n",
            "Requirement already satisfied: setuptools>=61.0 in /usr/local/lib/python3.11/dist-packages (from incremental>=24.7.0->twisted) (75.2.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.4.26)\n",
            "Downloading scrapy-2.13.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: pydispatcher, zope-interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed automat-25.4.16 constantly-23.10.4 cssselect-1.3.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 pydispatcher-2.0.7 queuelib-1.8.0 requests-file-2.1.0 scrapy-2.13.0 service-identity-24.2.0 tldextract-5.3.0 twisted-24.11.0 w3lib-2.3.1 zope-interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aljazeera_spider.py\n",
        "import scrapy\n",
        "import time\n",
        "\n",
        "class DelayMiddleware:\n",
        "    def __init__(self, delay=2):\n",
        "        self.delay = delay\n",
        "\n",
        "    @classmethod\n",
        "    def from_crawler(cls, crawler):\n",
        "        delay = crawler.settings.getfloat('DOWNLOAD_DELAY', 2)\n",
        "        return cls(delay)\n",
        "\n",
        "    def process_request(self, request, spider):\n",
        "        time.sleep(self.delay)\n",
        "\n",
        "class AljazeeraSpider(scrapy.Spider):\n",
        "    name = \"aljazeera\"\n",
        "    allowed_domains = [\"aljazeera.com\"]\n",
        "    start_urls = [\"https://www.aljazeera.com/news/\"]\n",
        "\n",
        "    custom_settings = {\n",
        "        'ROBOTSTXT_OBEY': True,\n",
        "        'DOWNLOAD_DELAY': 2,\n",
        "        'DOWNLOADER_MIDDLEWARES': {\n",
        "            '__main__.DelayMiddleware': 543,\n",
        "        },\n",
        "        'USER_AGENT': 'MyCrawlerBot/1.0 (+https://yourdomain.com)'\n",
        "    }\n",
        "\n",
        "    def parse(self, response):\n",
        "        for article in response.css('article'):\n",
        "            title = article.css('h2 a::text').get()\n",
        "            link = article.css('h2 a::attr(href)').get()\n",
        "            if title and link:\n",
        "                yield {\n",
        "                    'title': title.strip(),\n",
        "                    'link': response.urljoin(link)\n",
        "                }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX50pfXMJ6nJ",
        "outputId": "24186ac8-0e3d-4ed9-86d8-88e39af5da6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aljazeera_spider.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy runspider aljazeera_spider.py -o results.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vU1pJvRKAsO",
        "outputId": "894fc3d7-32fa-4f79-e631-17013afd0396"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-19 19:48:30 [scrapy.utils.log] INFO: Scrapy 2.13.0 started (bot: scrapybot)\n",
            "2025-05-19 19:48:30 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-05-19 19:48:30 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-05-19 19:48:30 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-05-19 19:48:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-05-19 19:48:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-05-19 19:48:30 [scrapy.extensions.telnet] INFO: Telnet Password: 7fdc3013d7187961\n",
            "2025-05-19 19:48:30 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-05-19 19:48:30 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'DOWNLOAD_DELAY': 2,\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_LOADER_WARN_ONLY': True,\n",
            " 'USER_AGENT': 'MyCrawlerBot/1.0 (+https://yourdomain.com)'}\n",
            "Unhandled error in Deferred:\n",
            "2025-05-19 19:48:30 [twisted] CRITICAL: Unhandled error in Deferred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/twisted/internet/defer.py\", line 2017, in _inlineCallbacks\n",
            "    result = context.run(gen.send, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 156, in crawl\n",
            "    self.engine = self._create_engine()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 169, in _create_engine\n",
            "    return ExecutionEngine(self, lambda _: self.stop())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 113, in __init__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 173, in close\n",
            "    self.downloader.close()\n",
            "builtins.AttributeError: 'ExecutionEngine' object has no attribute 'downloader'\n",
            "\n",
            "2025-05-19 19:48:30 [twisted] CRITICAL: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/misc.py\", line 74, in load_object\n",
            "    obj = getattr(mod, name)\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module '__main__' has no attribute 'DelayMiddleware'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 110, in __init__\n",
            "    self.downloader: Downloader = downloader_cls(crawler)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/downloader/__init__.py\", line 109, in __init__\n",
            "    DownloaderMiddlewareManager.from_crawler(crawler)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/middleware.py\", line 77, in from_crawler\n",
            "    return cls._from_settings(crawler.settings, crawler)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/middleware.py\", line 86, in _from_settings\n",
            "    mwcls = load_object(clspath)\n",
            "            ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/misc.py\", line 76, in load_object\n",
            "    raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n",
            "NameError: Module '__main__' doesn't define any object named 'DelayMiddleware'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/twisted/internet/defer.py\", line 2017, in _inlineCallbacks\n",
            "    result = context.run(gen.send, result)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 156, in crawl\n",
            "    self.engine = self._create_engine()\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 169, in _create_engine\n",
            "    return ExecutionEngine(self, lambda _: self.stop())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 113, in __init__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 173, in close\n",
            "    self.downloader.close()\n",
            "    ^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ExecutionEngine' object has no attribute 'downloader'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy runspider aljazeera_spider.py -o results.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zmVK4Z8KGkA",
        "outputId": "9d78528b-66d2-4088-b5af-7a6d28fbbb0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-19 19:48:33 [scrapy.utils.log] INFO: Scrapy 2.13.0 started (bot: scrapybot)\n",
            "2025-05-19 19:48:33 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-05-19 19:48:33 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-05-19 19:48:33 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-05-19 19:48:33 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-05-19 19:48:33 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-05-19 19:48:33 [scrapy.extensions.telnet] INFO: Telnet Password: bf54451ca9a85f1b\n",
            "2025-05-19 19:48:33 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-05-19 19:48:33 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'DOWNLOAD_DELAY': 2,\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_LOADER_WARN_ONLY': True,\n",
            " 'USER_AGENT': 'MyCrawlerBot/1.0 (+https://yourdomain.com)'}\n",
            "Unhandled error in Deferred:\n",
            "2025-05-19 19:48:33 [twisted] CRITICAL: Unhandled error in Deferred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/twisted/internet/defer.py\", line 2017, in _inlineCallbacks\n",
            "    result = context.run(gen.send, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 156, in crawl\n",
            "    self.engine = self._create_engine()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 169, in _create_engine\n",
            "    return ExecutionEngine(self, lambda _: self.stop())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 113, in __init__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 173, in close\n",
            "    self.downloader.close()\n",
            "builtins.AttributeError: 'ExecutionEngine' object has no attribute 'downloader'\n",
            "\n",
            "2025-05-19 19:48:33 [twisted] CRITICAL: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/misc.py\", line 74, in load_object\n",
            "    obj = getattr(mod, name)\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module '__main__' has no attribute 'DelayMiddleware'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 110, in __init__\n",
            "    self.downloader: Downloader = downloader_cls(crawler)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/downloader/__init__.py\", line 109, in __init__\n",
            "    DownloaderMiddlewareManager.from_crawler(crawler)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/middleware.py\", line 77, in from_crawler\n",
            "    return cls._from_settings(crawler.settings, crawler)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/middleware.py\", line 86, in _from_settings\n",
            "    mwcls = load_object(clspath)\n",
            "            ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/misc.py\", line 76, in load_object\n",
            "    raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n",
            "NameError: Module '__main__' doesn't define any object named 'DelayMiddleware'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/twisted/internet/defer.py\", line 2017, in _inlineCallbacks\n",
            "    result = context.run(gen.send, result)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 156, in crawl\n",
            "    self.engine = self._create_engine()\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 169, in _create_engine\n",
            "    return ExecutionEngine(self, lambda _: self.stop())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 113, in __init__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 173, in close\n",
            "    self.downloader.close()\n",
            "    ^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ExecutionEngine' object has no attribute 'downloader'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_settings = {\n",
        "    'DOWNLOAD_DELAY': 2,   # 2 seconds delay\n",
        "    'ROBOTSTXT_OBEY': True,\n",
        "    'USER_AGENT': 'MyCrawlerBot/1.0 (+https://yourdomain.com)'\n",
        "}\n"
      ],
      "metadata": {
        "id": "JYP63iVwKZT8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class AljazeeraSpider(scrapy.Spider):\n",
        "    name = 'aljazeera'\n",
        "    allowed_domains = ['aljazeera.com']\n",
        "    start_urls = ['https://www.aljazeera.com/xml/rss/all.xml']\n",
        "\n",
        "    custom_settings = {\n",
        "        'DOWNLOAD_DELAY': 2,\n",
        "        'ROBOTSTXT_OBEY': True,\n",
        "        'USER_AGENT': 'MyCrawlerBot/1.0 (+https://yourdomain.com)'\n",
        "    }\n",
        "\n",
        "    def parse(self, response):\n",
        "        for article in response.css('article'):\n",
        "            title = article.css('h2 a::text').get()\n",
        "            link = article.css('h2 a::attr(href)').get()\n",
        "            if title and link:\n",
        "                yield {\n",
        "                    'title': title.strip(),\n",
        "                    'link': response.urljoin(link)\n",
        "                }\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "FqbBQuzHK7oz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy runspider aljazeera_spider.py -o results.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THWrHpM_K7rf",
        "outputId": "be78cc1b-bf90-4b9d-bd7c-0f9600a68647"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-19 19:48:58 [scrapy.utils.log] INFO: Scrapy 2.13.0 started (bot: scrapybot)\n",
            "2025-05-19 19:48:58 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-05-19 19:48:58 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-05-19 19:48:58 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-05-19 19:48:58 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-05-19 19:48:58 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-05-19 19:48:58 [scrapy.extensions.telnet] INFO: Telnet Password: 83233fdca0c69ba7\n",
            "2025-05-19 19:48:58 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-05-19 19:48:58 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'DOWNLOAD_DELAY': 2,\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_LOADER_WARN_ONLY': True,\n",
            " 'USER_AGENT': 'MyCrawlerBot/1.0 (+https://yourdomain.com)'}\n",
            "Unhandled error in Deferred:\n",
            "2025-05-19 19:48:58 [twisted] CRITICAL: Unhandled error in Deferred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/twisted/internet/defer.py\", line 2017, in _inlineCallbacks\n",
            "    result = context.run(gen.send, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 156, in crawl\n",
            "    self.engine = self._create_engine()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 169, in _create_engine\n",
            "    return ExecutionEngine(self, lambda _: self.stop())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 113, in __init__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 173, in close\n",
            "    self.downloader.close()\n",
            "builtins.AttributeError: 'ExecutionEngine' object has no attribute 'downloader'\n",
            "\n",
            "2025-05-19 19:48:58 [twisted] CRITICAL: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/misc.py\", line 74, in load_object\n",
            "    obj = getattr(mod, name)\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module '__main__' has no attribute 'DelayMiddleware'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 110, in __init__\n",
            "    self.downloader: Downloader = downloader_cls(crawler)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/downloader/__init__.py\", line 109, in __init__\n",
            "    DownloaderMiddlewareManager.from_crawler(crawler)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/middleware.py\", line 77, in from_crawler\n",
            "    return cls._from_settings(crawler.settings, crawler)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/middleware.py\", line 86, in _from_settings\n",
            "    mwcls = load_object(clspath)\n",
            "            ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/misc.py\", line 76, in load_object\n",
            "    raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n",
            "NameError: Module '__main__' doesn't define any object named 'DelayMiddleware'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/twisted/internet/defer.py\", line 2017, in _inlineCallbacks\n",
            "    result = context.run(gen.send, result)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 156, in crawl\n",
            "    self.engine = self._create_engine()\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\", line 169, in _create_engine\n",
            "    return ExecutionEngine(self, lambda _: self.stop())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 113, in __init__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/engine.py\", line 173, in close\n",
            "    self.downloader.close()\n",
            "    ^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ExecutionEngine' object has no attribute 'downloader'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "qe8jsi5wXUeL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL_list = [\n",
        "    \"https://www.aljazeera.com/news/\",\n",
        "    \"https://www.aljazeera.com/economy/\",\n",
        "    \"https://www.aljazeera.com/sports/\",\n",
        "    \"https://www.aljazeera.com/videos/\",\n",
        "    \"https://www.aljazeera.com/tag/human-rights/\"\n",
        "    #'https://finance.yahoo.com/quote/7545.T/?p=7545.T',\n",
        "    #'https://finance.yahoo.com/quote/7447.T/?p=7447.T'\n",
        "\n",
        "]\n",
        "\n",
        "page_titles = []\n",
        "section_names = []\n",
        "\n",
        "j = 0\n",
        "while j < len(URL_list):\n",
        "    try:\n",
        "        time.sleep(2)\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        r = requests.get(URL_list[j], headers=headers)\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "        title = soup.title.text.strip()\n",
        "        page_titles.append(title)\n",
        "\n",
        "        section_name = URL_list[j].split(\"/\")[-2]\n",
        "        section_names.append(section_name)\n",
        "\n",
        "        print(j, \"-->\", title)\n",
        "        print('Done ')\n",
        "        j += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        page_titles.append(\"N/A\")\n",
        "        section_names.append(URL_list[j].split(\"/\")[-2])\n",
        "        print(j, \"--> failed \")\n",
        "        print(\"Error:\", e)\n",
        "        j += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYODtxYSagaL",
        "outputId": "b68531dc-469f-4b61-87bb-b15a3b9a760c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 --> News | Today's latest from Al Jazeera\n",
            "Done \n",
            "1 --> Economy | Today's latest from Al Jazeera\n",
            "Done \n",
            "2 --> Sport | Today's latest from Al Jazeera\n",
            "Done \n",
            "3 --> Video | Today's latest from Al Jazeera\n",
            "Done \n",
            "4 --> Human Rights | Today's latest from Al Jazeera\n",
            "Done \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from urllib.robotparser import RobotFileParser\n",
        "\n",
        "#url = \"https://www.aljazeera.com/search/article\"\n",
        "url = \"https://www.aljazeera.com/news/\"\n",
        "user_agent = \"MyBot/1.0\"\n",
        "\n",
        "robots_url = \"https://www.aljazeera.com/robots.txt\"\n",
        "rp = RobotFileParser()\n",
        "rp.set_url(robots_url)\n",
        "rp.read()\n",
        "\n",
        "if rp.can_fetch(user_agent, url):\n",
        "    delay = rp.crawl_delay(user_agent)\n",
        "    if delay is None:\n",
        "        delay = 2\n",
        "\n",
        "    print(f\"Crawling {url} with delay {delay}s\")\n",
        "\n",
        "    response = requests.get(url, headers={\"User-Agent\": user_agent})\n",
        "    if response.status_code == 200:\n",
        "        print(\"Page fetched successfully\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {response.status_code}\")\n",
        "\n",
        "    time.sleep(delay)\n",
        "else:\n",
        "    print(\"Fetching disallowed by robots.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74i6RnGEl6vS",
        "outputId": "ec6a0489-36e2-414a-8250-1e0136fa275a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling https://www.aljazeera.com/news/ with delay 2s\n",
            "Page fetched successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium fake-useragent webdriver-manager\n",
        "!apt-get update > /dev/null\n",
        "!apt install chromium-chromedriver > /dev/null\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr3893WEGN_9",
        "outputId": "0c162a3a-1482-4e2e-8001-94f3911607d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, python-dotenv, outcome, fake-useragent, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed fake-useragent-2.2.0 outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 1: Install dependencies ------------------\n",
        "!pip install selenium fake-useragent webdriver-manager > /dev/null\n",
        "!apt-get update > /dev/null\n",
        "!apt install chromium-chromedriver > /dev/null\n",
        "\n",
        "# ------------------ STEP 2: Setup environment ------------------\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "os.environ['PATH'] += ':/usr/lib/chromium-browser:/usr/bin/chromedriver'\n",
        "\n",
        "# ------------------ STEP 3: Utility Functions ------------------\n",
        "def get_user_agent():\n",
        "    try:\n",
        "        ua = UserAgent()\n",
        "        return ua.random\n",
        "    except:\n",
        "        return \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "\n",
        "def get_selenium_options():\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument('--disable-infobars')\n",
        "    options.add_argument('--disable-extensions')\n",
        "    options.add_argument('--window-size=1920x1080')\n",
        "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    options.add_argument(f'user-agent={get_user_agent()}')\n",
        "    options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
        "    options.add_experimental_option('useAutomationExtension', False)\n",
        "    return options\n",
        "\n",
        "def remove_webdriver_flag(driver):\n",
        "    driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
        "        \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
        "    })\n",
        "\n",
        "def scroll_down(driver, scroll_pause=2, max_scrolls=5):\n",
        "    for i in range(max_scrolls):\n",
        "        print(f\"🌀 Scrolling page ({i+1}/{max_scrolls})...\")\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(scroll_pause)\n",
        "\n",
        "def click_load_more(driver, max_clicks=3):\n",
        "    for i in range(max_clicks):\n",
        "        try:\n",
        "            load_more = WebDriverWait(driver, 5).until(\n",
        "                EC.element_to_be_clickable((By.XPATH, '//button[contains(text(),\"المزيد\")]'))\n",
        "            )\n",
        "            print(f\"🔘 Clicking 'Load More' button ({i+1}/{max_clicks})\")\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView();\", load_more)\n",
        "            load_more.click()\n",
        "            time.sleep(3)\n",
        "        except:\n",
        "            print(\"ℹ No more 'Load More' button or failed to click.\")\n",
        "            break\n",
        "\n",
        "def extract_article_links(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    article_links = []\n",
        "    for link in soup.select(\"article a\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href and href.startswith(\"/\"):\n",
        "            article_links.append(\"https://www.aljazeera.com\" + href)\n",
        "    return list(set(article_links))\n",
        "\n",
        "def save_links_to_csv(links, page_title, filename):\n",
        "    try:\n",
        "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"Page Title\", page_title])\n",
        "            writer.writerow([\"URL\"])\n",
        "            for link in links:\n",
        "                writer.writerow([link])\n",
        "        print(f\"📥 Saved {len(links)} article links to '{filename}'\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Error while saving file:\", str(e))\n",
        "\n",
        "def selenium_scrape_with_retry(url, max_retries=3):\n",
        "    options = get_selenium_options()\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        print(f\"🔁 Attempt {attempt} to load: {url}\")\n",
        "        driver = webdriver.Chrome(service=Service(), options=options)\n",
        "        remove_webdriver_flag(driver)\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            WebDriverWait(driver, 10).until(EC.title_contains(\"Al Jazeera\"))\n",
        "            page_title = driver.title\n",
        "            print(\"📄 Page title after load:\", page_title)\n",
        "            scroll_down(driver)\n",
        "            click_load_more(driver)\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"article\"))\n",
        "            )\n",
        "            print(\"✅ Page fully loaded with JS content\")\n",
        "            content = driver.page_source\n",
        "            article_links = extract_article_links(content)\n",
        "\n",
        "            # Debug output\n",
        "            print(f\"🔗 Found {len(article_links)} article links.\")\n",
        "            for link in article_links:\n",
        "                print(link)\n",
        "\n",
        "            # Save links\n",
        "            save_path = \"/content/aljazeera_article_links_scraped.csv\"\n",
        "            save_links_to_csv(article_links, page_title, filename=save_path)\n",
        "            driver.quit()\n",
        "            return article_links\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during attempt {attempt}: {e}\")\n",
        "            driver.quit()\n",
        "            if attempt < max_retries:\n",
        "                print(\"⏳ Retrying in 5 seconds...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                print(\"🚫 Maximum retry attempts reached. Exiting.\")\n",
        "                return []\n",
        "\n",
        "# ------------------ STEP 4: Requests Part ------------------\n",
        "BASE_URL = \"https://www.aljazeera.com\"\n",
        "session = requests.Session()\n",
        "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "adapter = HTTPAdapter(max_retries=retries)\n",
        "session.mount(\"http://\", adapter)\n",
        "session.mount(\"https://\", adapter)\n",
        "\n",
        "def scrape_article_details(url):\n",
        "    response = session.get(url)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No title\"\n",
        "\n",
        "    date_tag = soup.find(\"time\")\n",
        "    date = date_tag.get_text(strip=True) if date_tag else \"No date\"\n",
        "\n",
        "    author_tag = soup.find(lambda tag: tag.name in ['span', 'div'] and 'author' in tag.get('class', []))\n",
        "    author = author_tag.get_text(strip=True) if author_tag else \"No author\"\n",
        "\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "    article_text = article_text[:50] + \"...\" if len(article_text) > 200 else article_text\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"date\": date,\n",
        "        \"author\": author,\n",
        "        \"text\": article_text\n",
        "    }\n",
        "\n",
        "# ------------------ STEP 5: Run the whole thing ------------------\n",
        "def main():\n",
        "    start_url = \"https://www.aljazeera.com\"\n",
        "    print(\"Starting Selenium scraping of main page to get article links...\")\n",
        "    selenium_links = selenium_scrape_with_retry(start_url)\n",
        "\n",
        "    print(\"Total article links found by Selenium:\", len(selenium_links))\n",
        "\n",
        "    articles_data = []\n",
        "    seen_links = set()\n",
        "    MAX_ARTICLES = 300\n",
        "\n",
        "    for link in selenium_links:\n",
        "        if link in seen_links:\n",
        "            continue\n",
        "        if len(articles_data) >= MAX_ARTICLES:\n",
        "            break\n",
        "        print(f\"Scraping article details: {link}\")\n",
        "        try:\n",
        "            article = scrape_article_details(link)\n",
        "            articles_data.append(article)\n",
        "            seen_links.add(link)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {link}: {e}\")\n",
        "\n",
        "    print(f\"Total articles scraped: {len(articles_data)}\")\n",
        "\n",
        "    output_filename = \"/content/aljazeera_detailed_articles.csv\"\n",
        "    keys = [\"url\", \"title\", \"date\", \"author\", \"text\"]\n",
        "    try:\n",
        "        with open(output_filename, \"w\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(articles_data)\n",
        "        print(f\"Saved detailed articles to {output_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving detailed articles file: {e}\")\n",
        "\n",
        "# Run it\n",
        "main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo4RShD3HGCQ",
        "outputId": "87a63012-ead9-4a9b-8547-908551c193f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Starting Selenium scraping of main page to get article links...\n",
            "🔁 Attempt 1 to load: https://www.aljazeera.com\n",
            "📄 Page title after load: Breaking News, World News and Video from Al Jazeera\n",
            "🌀 Scrolling page (1/5)...\n",
            "🌀 Scrolling page (2/5)...\n",
            "🌀 Scrolling page (3/5)...\n",
            "🌀 Scrolling page (4/5)...\n",
            "🌀 Scrolling page (5/5)...\n",
            "ℹ No more 'Load More' button or failed to click.\n",
            "✅ Page fully loaded with JS content\n",
            "🔗 Found 86 article links.\n",
            "https://www.aljazeera.com/features/longform/2025/5/19/a-fathers-fight-to-find-out-what-happened-to-his-son-who-joined-isis\n",
            "https://www.aljazeera.com/news/2025/5/18/pope-leo-xiv-meets-ukraines-zelenskyy-after-his-inaugural-mass\n",
            "https://www.aljazeera.com/gallery/2025/5/19/dr-congos-coltan-miners-struggle-as-they-dig-to-feed-worlds-tech\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720778\n",
            "https://www.aljazeera.com/sports/2025/5/19/thunder-vs-nuggets-okc-win-game-7-reach-western-conference-finals\n",
            "https://www.aljazeera.com/news/2025/5/18/mexican-navy-ship-crashes-into-brooklyn-bridge-in-new-york-nineteen-injured\n",
            "https://www.aljazeera.com/news/2025/5/18/eu-uk-leaders-to-speak-with-trump-before-his-putin-call-as-ukraine-hit\n",
            "https://www.aljazeera.com/features/2025/5/17/exploding-inequality-the-fight-for-the-hearts-and-minds-of-polands-left\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital\n",
            "https://www.aljazeera.com/economy/2025/5/19/chinas-industrial-output-retail-sales-dip-amid-us-trade-tensions\n",
            "https://www.aljazeera.com/news/2025/5/17/irans-leaders-slam-trump-for-disgraceful-remarks-during-middle-east-tour\n",
            "https://www.aljazeera.com/news/2025/5/19/military-and-economic-deals-in-pipeline-as-uk-looks-to-eu-reset\n",
            "https://www.aljazeera.com/news/2025/5/18/pro-eu-nicusor-dan-on-course-to-beat-hard-right-rival-in-romania-election\n",
            "https://www.aljazeera.com/news/2025/5/18/at-least-10-reported-killed-in-suicide-bomb-blast-in-somalias-mogadishu\n",
            "https://www.aljazeera.com/sports/2025/5/16/india-neeraj-chopra-javelin-olympics-record-arshad-nadeem-world-athletics-doha-diamond-league\n",
            "https://www.aljazeera.com/news/2025/5/16/breaking-down-a-deadly-week-in-gaza-as-israel-kills-hundreds\n",
            "https://www.aljazeera.com/news/2025/5/17/gaza-likely-to-dominate-agenda-as-arab-league-meets-in-baghdad\n",
            "https://www.aljazeera.com/news/2025/5/18/indias-space-agency-suffers-setback-as-it-fails-to-launch-satellite\n",
            "https://www.aljazeera.com/news/2025/5/17/hamas-says-new-gaza-truce-talks-under-way-as-israel-expands-ground-assault\n",
            "https://www.aljazeera.com/news/2025/5/19/trumps-massive-tax-cut-bill-passes-key-us-house-committee-vote\n",
            "https://www.aljazeera.com/opinions/2025/5/16/in-istanbul-russia-plays-chess-while-the-west-is-stuck-in-make-believe\n",
            "https://www.aljazeera.com/news/2025/5/17/fight-back-pedro-pascal-urges-cannes-to-resist-trump-policies\n",
            "https://www.aljazeera.com/opinions/2025/5/15/from-1948-to-now-a-nakba-that-never-ended\n",
            "https://www.aljazeera.com/news/2025/5/19/gazas-khan-younis-latest-focus-of-israeli-forced-displacement-bombing\n",
            "https://www.aljazeera.com/news/2025/5/19/rains-halt-search-for-gold-miners-after-deadly-indonesian-landslide\n",
            "https://www.aljazeera.com/sports/2025/5/17/fa-cup-final-2025-crystal-palace-defeat-man-city-in-major-upset\n",
            "https://www.aljazeera.com/opinions/2025/5/18/trumps-tariffs-are-failing-but-the-old-model-wont-save-us-either\n",
            "https://www.aljazeera.com/news/2025/5/17/one-killed-after-explosion-near-fertility-clinic-in-palm-springs\n",
            "https://www.aljazeera.com/news/2025/5/19/car-bomb-in-pakistans-restive-southwest-kills-four-people\n",
            "https://www.aljazeera.com/news/2025/5/19/russia-ukraine-war-list-of-key-events-day-1180\n",
            "https://www.aljazeera.com/gallery/2025/5/18/pro-palestinian-protesters-rally-around-the-world-to-mark-nakba-day\n",
            "https://www.aljazeera.com/news/2025/5/17/tornadoes-kill-at-least-21-in-southern-us-states-of-missouri-and-kentucky\n",
            "https://www.aljazeera.com/news/2025/5/19/romania-poland-portugal-election-results-who-are-the-winners-and-losers\n",
            "https://www.aljazeera.com/news/2025/5/18/portugal-holds-its-third-election-in-three-years\n",
            "https://www.aljazeera.com/news/2025/5/19/moscow-outlaws-amnesty-international-for-russophobia-amid-ukraine-war\n",
            "https://www.aljazeera.com/sports/2025/5/19/us-pga-championship-2025-scottie-scheffler-wins-third-major-title\n",
            "https://www.aljazeera.com/news/2025/5/17/ukraine-says-russian-drone-attack-on-bus-kills-9-hours-after-direct-talks\n",
            "https://www.aljazeera.com/features/2025/5/19/fear-is-real-why-are-kashmiri-youths-removing-tattoos-from-their-bodies\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720821\n",
            "https://www.aljazeera.com/sports/2025/5/18/verstappen-beats-norris-piastri-in-f1-emilia-romagna-gp-at-imola\n",
            "https://www.aljazeera.com/sports/2025/5/17/q-african-sprinters\n",
            "https://www.aljazeera.com/news/2025/5/18/deadly-blast-rocks-police-station-in-eastern-syria-killing-three-report\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720769\n",
            "https://www.aljazeera.com/opinions/2025/5/16/seventy-seven-years-after-the-nakba-we-are-naming-our-new-ruin\n",
            "https://www.aljazeera.com/news/2025/5/19/iran-summons-uk-charge-daffaires-amid-nuclear-friction\n",
            "https://www.aljazeera.com/news/2025/5/19/gleason-score-9-how-aggressive-is-bidens-prostrate-cancer\n",
            "https://www.aljazeera.com/news/2025/5/18/children-among-over-100-palestinians-killed-in-israeli-barrage-across-gaza\n",
            "https://www.aljazeera.com/news/2025/5/18/russia-detains-greek-oil-tanker-after-it-departs-estonian-port\n",
            "https://www.aljazeera.com/news/2025/5/18/portuguese-pms-party-set-to-win-general-election-fall-short-of-majority\n",
            "https://www.aljazeera.com/news/2025/5/17/at-least-three-killed-as-syrian-forces-raid-isil-hideouts-in-aleppo\n",
            "https://www.aljazeera.com/news/2025/5/19/trump-putin-call-whats-holding-up-a-russia-ukraine-ceasefire\n",
            "https://www.aljazeera.com/video/inside-story/2025/5/19/europes-political-centre-holds-in-weekend-of-elections\n",
            "https://www.aljazeera.com/author/belen_fernandez_201163082655120314\n",
            "https://www.aljazeera.com/news/2025/5/19/uk-eu-reach-landmark-deal-all-you-need-to-know\n",
            "https://www.aljazeera.com/news/2025/5/19/south-africas-ramaphosa-to-visit-trump-can-they-fix-tricky-relations\n",
            "https://www.aljazeera.com/sports/2025/5/17/knicks-oust-celtics-in-game-6-playoffs-make-eastern-conference-finals\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720797\n",
            "https://www.aljazeera.com/gallery/2025/5/18/week-in-pictures-deadly-storms-in-the-us-to-forced-displacement-in-gaza\n",
            "https://www.aljazeera.com/news/2025/5/17/malaysias-fish-hunters-target-invasive-species-one-catch-at-a-time\n",
            "https://www.aljazeera.com/sports/2025/5/19/gary-lineker-leaves-bbc-amid-anti-semitism-row-pro-palestinian-comments\n",
            "https://www.aljazeera.com/news/2025/5/16/poland-presidential-election-2025-from-migration-to-eu-whats-at-stake\n",
            "https://www.aljazeera.com/news/2025/5/19/chechnyas-kadyrov-wanted-to-resign-or-did-he\n",
            "https://www.aljazeera.com/news/2025/5/18/indian-professor-arrested-over-social-media-post-on-military-operation\n",
            "https://www.aljazeera.com/opinions/2025/5/19/cheer-up-people-of-gaza-youll-get-killed-on-a-full-stomach\n",
            "https://www.aljazeera.com/news/2025/5/19/israel-to-allow-limited-food-into-gaza-amid-intensified-military-offensive\n",
            "https://www.aljazeera.com/news/2025/5/16/has-india-offered-trump-zero-tariffs-what-we-know-and-why-it-matters\n",
            "https://www.aljazeera.com/news/2025/5/19/measure-targeting-pro-palestine-ngos-disappears-from-us-tax-bill\n",
            "https://www.aljazeera.com/sports/2025/5/19/lionel-messi-inter-miami-lose-to-orlando-city-in-mls\n",
            "https://www.aljazeera.com/news/2025/5/18/former-us-president-biden-diagnosed-with-aggressive-prostate-cancer\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720801\n",
            "https://www.aljazeera.com/gallery/2025/5/19/tens-of-thousands-march-in-the-netherlands-to-protest-against-gaza-genocide\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/russia-ukraine-war-live-trump-to-hold-calls-with-putin-zelenskyy\n",
            "https://www.aljazeera.com/news/2025/5/19/sudans-army-leader-al-burhan-appoints-former-un-official-as-prime-minister\n",
            "https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720864\n",
            "https://www.aljazeera.com/program/inside-story/\n",
            "https://www.aljazeera.com/author/ahmed-al-najjar\n",
            "https://www.aljazeera.com/news/2025/5/19/former-president-bolsonaros-coup-trial-opens-in-brazil\n",
            "https://www.aljazeera.com/news/2025/5/19/mexican-ship-hits-brooklyn-bridge-what-went-wrong-who-were-the-victims\n",
            "https://www.aljazeera.com/opinions/2025/5/19/project-esther-and-the-weaponisation-of-zionism\n",
            "https://www.aljazeera.com/news/2025/5/19/tanzanian-opposition-leader-makes-defiant-appearance-at-treason-trial\n",
            "https://www.aljazeera.com/economy/2025/5/19/regeneron-buys-23andme-for-256m-after-bankruptcy\n",
            "https://www.aljazeera.com/news/2025/5/19/lithuania-files-case-against-belarus-at-icj-over-alleged-people-smuggling\n",
            "https://www.aljazeera.com/news/2025/5/18/israeli-strikes-batter-gaza-hospitals-as-brutal-siege-bombing-intensify\n",
            "https://www.aljazeera.com/news/2025/5/18/centrist-trzaskowski-leads-first-round-in-polands-tight-presidential-poll\n",
            "https://www.aljazeera.com/news/2025/5/18/south-koreas-presidential-candidates-hold-first-heated-debate\n",
            "https://www.aljazeera.com/features/2025/5/18/could-ai-help-elderly-people-and-refugees-reconstruct-their-unrecorded-pasts\n",
            "📥 Saved 86 article links to '/content/aljazeera_article_links_scraped.csv'\n",
            "Total article links found by Selenium: 86\n",
            "Scraping article details: https://www.aljazeera.com/features/longform/2025/5/19/a-fathers-fight-to-find-out-what-happened-to-his-son-who-joined-isis\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/pope-leo-xiv-meets-ukraines-zelenskyy-after-his-inaugural-mass\n",
            "Scraping article details: https://www.aljazeera.com/gallery/2025/5/19/dr-congos-coltan-miners-struggle-as-they-dig-to-feed-worlds-tech\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720778\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/19/thunder-vs-nuggets-okc-win-game-7-reach-western-conference-finals\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/mexican-navy-ship-crashes-into-brooklyn-bridge-in-new-york-nineteen-injured\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/eu-uk-leaders-to-speak-with-trump-before-his-putin-call-as-ukraine-hit\n",
            "Scraping article details: https://www.aljazeera.com/features/2025/5/17/exploding-inequality-the-fight-for-the-hearts-and-minds-of-polands-left\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital\n",
            "Scraping article details: https://www.aljazeera.com/economy/2025/5/19/chinas-industrial-output-retail-sales-dip-amid-us-trade-tensions\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/irans-leaders-slam-trump-for-disgraceful-remarks-during-middle-east-tour\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/military-and-economic-deals-in-pipeline-as-uk-looks-to-eu-reset\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/pro-eu-nicusor-dan-on-course-to-beat-hard-right-rival-in-romania-election\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/at-least-10-reported-killed-in-suicide-bomb-blast-in-somalias-mogadishu\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/16/india-neeraj-chopra-javelin-olympics-record-arshad-nadeem-world-athletics-doha-diamond-league\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/16/breaking-down-a-deadly-week-in-gaza-as-israel-kills-hundreds\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/gaza-likely-to-dominate-agenda-as-arab-league-meets-in-baghdad\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/indias-space-agency-suffers-setback-as-it-fails-to-launch-satellite\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/hamas-says-new-gaza-truce-talks-under-way-as-israel-expands-ground-assault\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/trumps-massive-tax-cut-bill-passes-key-us-house-committee-vote\n",
            "Scraping article details: https://www.aljazeera.com/opinions/2025/5/16/in-istanbul-russia-plays-chess-while-the-west-is-stuck-in-make-believe\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/fight-back-pedro-pascal-urges-cannes-to-resist-trump-policies\n",
            "Scraping article details: https://www.aljazeera.com/opinions/2025/5/15/from-1948-to-now-a-nakba-that-never-ended\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/gazas-khan-younis-latest-focus-of-israeli-forced-displacement-bombing\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/rains-halt-search-for-gold-miners-after-deadly-indonesian-landslide\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/17/fa-cup-final-2025-crystal-palace-defeat-man-city-in-major-upset\n",
            "Scraping article details: https://www.aljazeera.com/opinions/2025/5/18/trumps-tariffs-are-failing-but-the-old-model-wont-save-us-either\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/one-killed-after-explosion-near-fertility-clinic-in-palm-springs\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/car-bomb-in-pakistans-restive-southwest-kills-four-people\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/russia-ukraine-war-list-of-key-events-day-1180\n",
            "Scraping article details: https://www.aljazeera.com/gallery/2025/5/18/pro-palestinian-protesters-rally-around-the-world-to-mark-nakba-day\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/tornadoes-kill-at-least-21-in-southern-us-states-of-missouri-and-kentucky\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/romania-poland-portugal-election-results-who-are-the-winners-and-losers\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/portugal-holds-its-third-election-in-three-years\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/moscow-outlaws-amnesty-international-for-russophobia-amid-ukraine-war\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/19/us-pga-championship-2025-scottie-scheffler-wins-third-major-title\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/ukraine-says-russian-drone-attack-on-bus-kills-9-hours-after-direct-talks\n",
            "Scraping article details: https://www.aljazeera.com/features/2025/5/19/fear-is-real-why-are-kashmiri-youths-removing-tattoos-from-their-bodies\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720821\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/18/verstappen-beats-norris-piastri-in-f1-emilia-romagna-gp-at-imola\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/17/q-african-sprinters\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/deadly-blast-rocks-police-station-in-eastern-syria-killing-three-report\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720769\n",
            "Scraping article details: https://www.aljazeera.com/opinions/2025/5/16/seventy-seven-years-after-the-nakba-we-are-naming-our-new-ruin\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/iran-summons-uk-charge-daffaires-amid-nuclear-friction\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/gleason-score-9-how-aggressive-is-bidens-prostrate-cancer\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/children-among-over-100-palestinians-killed-in-israeli-barrage-across-gaza\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/russia-detains-greek-oil-tanker-after-it-departs-estonian-port\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/portuguese-pms-party-set-to-win-general-election-fall-short-of-majority\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/at-least-three-killed-as-syrian-forces-raid-isil-hideouts-in-aleppo\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/trump-putin-call-whats-holding-up-a-russia-ukraine-ceasefire\n",
            "Scraping article details: https://www.aljazeera.com/video/inside-story/2025/5/19/europes-political-centre-holds-in-weekend-of-elections\n",
            "Scraping article details: https://www.aljazeera.com/author/belen_fernandez_201163082655120314\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/uk-eu-reach-landmark-deal-all-you-need-to-know\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/south-africas-ramaphosa-to-visit-trump-can-they-fix-tricky-relations\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/17/knicks-oust-celtics-in-game-6-playoffs-make-eastern-conference-finals\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720797\n",
            "Scraping article details: https://www.aljazeera.com/gallery/2025/5/18/week-in-pictures-deadly-storms-in-the-us-to-forced-displacement-in-gaza\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/17/malaysias-fish-hunters-target-invasive-species-one-catch-at-a-time\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/19/gary-lineker-leaves-bbc-amid-anti-semitism-row-pro-palestinian-comments\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/16/poland-presidential-election-2025-from-migration-to-eu-whats-at-stake\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/chechnyas-kadyrov-wanted-to-resign-or-did-he\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/indian-professor-arrested-over-social-media-post-on-military-operation\n",
            "Scraping article details: https://www.aljazeera.com/opinions/2025/5/19/cheer-up-people-of-gaza-youll-get-killed-on-a-full-stomach\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/israel-to-allow-limited-food-into-gaza-amid-intensified-military-offensive\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/16/has-india-offered-trump-zero-tariffs-what-we-know-and-why-it-matters\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/measure-targeting-pro-palestine-ngos-disappears-from-us-tax-bill\n",
            "Scraping article details: https://www.aljazeera.com/sports/2025/5/19/lionel-messi-inter-miami-lose-to-orlando-city-in-mls\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/former-us-president-biden-diagnosed-with-aggressive-prostate-cancer\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720801\n",
            "Scraping article details: https://www.aljazeera.com/gallery/2025/5/19/tens-of-thousands-march-in-the-netherlands-to-protest-against-gaza-genocide\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/russia-ukraine-war-live-trump-to-hold-calls-with-putin-zelenskyy\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/sudans-army-leader-al-burhan-appoints-former-un-official-as-prime-minister\n",
            "Scraping article details: https://www.aljazeera.com/news/liveblog/2025/5/19/live-israel-kills-144-palestinians-targets-north-gaza-hospital?update=3720864\n",
            "Scraping article details: https://www.aljazeera.com/program/inside-story/\n",
            "Scraping article details: https://www.aljazeera.com/author/ahmed-al-najjar\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/former-president-bolsonaros-coup-trial-opens-in-brazil\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/mexican-ship-hits-brooklyn-bridge-what-went-wrong-who-were-the-victims\n",
            "Scraping article details: https://www.aljazeera.com/opinions/2025/5/19/project-esther-and-the-weaponisation-of-zionism\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/tanzanian-opposition-leader-makes-defiant-appearance-at-treason-trial\n",
            "Scraping article details: https://www.aljazeera.com/economy/2025/5/19/regeneron-buys-23andme-for-256m-after-bankruptcy\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/19/lithuania-files-case-against-belarus-at-icj-over-alleged-people-smuggling\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/israeli-strikes-batter-gaza-hospitals-as-brutal-siege-bombing-intensify\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/centrist-trzaskowski-leads-first-round-in-polands-tight-presidential-poll\n",
            "Scraping article details: https://www.aljazeera.com/news/2025/5/18/south-koreas-presidential-candidates-hold-first-heated-debate\n",
            "Scraping article details: https://www.aljazeera.com/features/2025/5/18/could-ai-help-elderly-people-and-refugees-reconstruct-their-unrecorded-pasts\n",
            "Total articles scraped: 86\n",
            "Saved detailed articles to /content/aljazeera_detailed_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/aljazeera_article_links_scraped.csv')\n",
        "files.download('/content/aljazeera_detailed_articles.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5xViyJE0HGEl",
        "outputId": "39772cef-6c7b-4143-ea3a-a320b632798d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0759e519-e8ac-425a-b902-3e9704741e9b\", \"aljazeera_article_links_scraped.csv\", 9405)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_efa08a3f-3083-4c5a-8b95-94663a770309\", \"aljazeera_detailed_articles.csv\", 22377)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}